{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Classifiers with Watson Visual Recognition\n",
    "\n",
    "First,  we import a bunch of things we're going to need.  You can install these with `pip`.  I recommend you create\n",
    "a virtual environment. \n",
    "\n",
    "If you are using Anaconda,  you can do `conda create --name watson_vr python=3`\n",
    "\n",
    "This will create a new environment named `watson_vr` which you can activate with `source activate watson_vr`\n",
    "and then you can run `pip install -r requirements.txt` to get all the libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from watson_developer_cloud import VisualRecognitionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You'll need a Visual Recognition API Key\n",
    "\n",
    "You can get that from Bluemix after you activate the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_KEY = os.getenv('API_KEY') or 'ENTER YOUR API KEY HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is just some helper code\n",
    "\n",
    "The following cell sets up some helper code I'll be using in this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositiveClass:\n",
    "    def __init__(self, class_name, tmpdir, urls):\n",
    "        self.tmpdir = tmpdir\n",
    "        self.class_name = class_name\n",
    "        self.urls = urls\n",
    "        \n",
    "    def name(self):\n",
    "        return self.class_name\n",
    "\n",
    "    def path(self):\n",
    "        return os.path.join(self.tmpdir.name,\n",
    "                            \"{0}_positive_examples.zip\".format(self.name()))\n",
    "        \n",
    "    def zipfile(self):\n",
    "        output_zipfile_name = self.path()\n",
    "        if os.path.isfile(output_zipfile_name):\n",
    "            return output_zipfile_name\n",
    "        \n",
    "        with ZipFile(output_zipfile_name, mode=\"w\") as output_zip:\n",
    "            for image in self.urls:\n",
    "                with open(image, 'rb') as imagefile:\n",
    "                    arc_name = os.path.basename(image)\n",
    "                    output_zip.writestr(arc_name, imagefile.read())\n",
    "        return output_zipfile_name\n",
    "\n",
    "class NegativeClass(PositiveClass):\n",
    "    def __init__(self, tmpdir, urls):\n",
    "        super().__init__(\"negative\", tmpdir, urls)\n",
    "        \n",
    "    def path(self):\n",
    "        return os.path.join(self.tmpdir.name,\"negative_examples.zip\")\n",
    "    \n",
    "def desc_stats(numbers):\n",
    "    return { 'std dev': np.std(numbers),\n",
    "    'mean': np.mean(numbers),\n",
    "     'min': np.min(numbers),\n",
    "     'max': np.max(numbers)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempDir = TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Magic Starts Here\n",
    "\n",
    "It's not really magic.  The heavy lifting here is done by a function from [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) that splist an array of objects into \n",
    "two arrays.  I just use the default, which is a `75/25` split (meaning 75% of the items are in the `train` set and 25% are in the test set).  This is all nicely randomized so your images are bucketed nicely.\n",
    "\n",
    "For this discussion, I'm using images from the [Watson Visual Recognition Demo](https://visual-recognition-demo.mybluemix.net) and I'm going to create a custom classifier with two classes:  `golden` and `beagle` which contain images of Golden Retrievers and Beagles respectively.  I'm also going to be supplying a set of images that are not in any of those two classes (See [the docs](https://www.ibm.com/watson/developercloud/doc/visual-recognition/customizing.html) for real details about how this all works).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "base_path_gr = \"./images/goldenretriever/\"\n",
    "train_gr, test_gr = train_test_split(os.listdir(base_path_gr))\n",
    "test_gr_images = [os.path.join(base_path_gr,x) for x in test_gr]\n",
    "\n",
    "base_path_beagle = \"./images/beagle/\"\n",
    "train_beagle, test_beagle = train_test_split(os.listdir(base_path_beagle))\n",
    "test_beagle_images = [os.path.join(base_path_beagle,x) for x in test_beagle]\n",
    "\n",
    "base_path_negative = \"./images/negatives/\"\n",
    "train_negative = os.listdir(base_path_negative)\n",
    "\n",
    "gr = PositiveClass(\"golden\", tempDir, [os.path.join(base_path_gr,x) for x in train_gr])\n",
    "beagle = PositiveClass(\"beagle\", tempDir, [os.path.join(base_path_beagle,x) for x in train_beagle])\n",
    "\n",
    "negative = NegativeClass(tempDir, [os.path.join(base_path_negative,x) for x in train_negative])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a client class and list the classifiers available just to make sure we can connect and everything is good.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = VisualRecognitionV3(version='2017-01-31', api_key=API_KEY)\n",
    "vr.list_classifiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To train a classifier, we use a zipfile of the training files and submit that to the service. \n",
    "\n",
    "Training takes a while,  so don't be surprised and while you're waiting you can See [the docs](https://www.ibm.com/watson/developercloud/doc/visual-recognition/customizing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grzip = open(gr.zipfile(), 'rb')\n",
    "beaglezip = open(beagle.zipfile(), 'rb')\n",
    "negativezip = open(negative.zipfile(), 'rb')\n",
    "\n",
    "created_id = vr.create_classifier(golden_positive_examples=grzip,\n",
    "                                  beagle_positive_examples=beaglezip,\n",
    "                                  negative_examples=negativezip,\n",
    "                                  name=\"POC_two\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though training takes a while, we get back the classifier ID immediately and will poll the service to find out when the training has finished.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier_id = created_id[\"classifier_id\"]\n",
    "\n",
    "while vr.get_classifier(created_id[\"classifier_id\"])['status'] == 'training':\n",
    "    sleep(10)\n",
    "\n",
    "created_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Once the classifier is training, we can test our images.  Since we have a batch of test images already from splitting the data earlier, we can just feed each one of these images to the system and get the results.  This will also take some time to complete.  \n",
    "\n",
    "This call will use both the custom classier created earlier and the default classifier that's always there.   In many applications, the tags returned can be used to infer other information about whats in the image (though that is not covered in this notebook).\n",
    "\n",
    "In these tests I'm passing in a `threshold` value of `0.8`.  This means only classes with a score `>` `0.8` will be returned. The default threshold is `0.5` but I wanted to limit the data returned so it shows up cleaning in the datatable below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_results = []\n",
    "\n",
    "test_image_files = test_beagle_images\n",
    "\n",
    "for image_file in test_image_files:\n",
    "    with open(image_file,'rb') as ifile:\n",
    "        result = vr.classify(images_file=ifile, classifier_ids=[my_classifier_id,'default'],threshold=0.8)\n",
    "        test_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just an example of how to classify a single image\n",
    "#with open(test_image_files[0], 'rb') as ifile:\n",
    "#    print(vr.classify(images_file=ifile, classifier_ids=[my_classifier_id,'default'],threshold=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our results,  this code will flatten the results for visualization.  The service returns a pretty dense JSON payload,  I'm only going to look at the classes for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_scores = {}\n",
    "# DANGER WILL ROBINSON:  the following is a set of\n",
    "# actions that flatten the list of classes returned by the classifiers\n",
    "scores_list = list(itertools.chain(*[m['classes'] \n",
    "                       for m \n",
    "                       in list(itertools.chain(*[z[0]['classifiers'] \n",
    "                                                 for z \n",
    "                                                 in [u['images'] \n",
    "                                                     for u \n",
    "                                                     in test_results]]))]))\n",
    "for y in scores_list:\n",
    "    if y['class'] in class_scores:\n",
    "        class_scores[y['class']].append(y['score'])\n",
    "    else:\n",
    "        class_scores[y['class']]=[y['score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have flattened the data,  you'll see a plot of the scores and a dataframe of the score results.  It's important to remember that the `score` is not a confidence score, or a percentage.  However, we use the scores for the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = class_scores.keys()\n",
    "\n",
    "for l in labels:\n",
    "    plt.scatter(range(0,len(class_scores[l])), class_scores[l], label=l)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "plt.title(\"Scores\")\n",
    "plt.show()\n",
    "\n",
    "pandas.DataFrame(dict([(x,desc_stats(class_scores[x])) for x in labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining\n",
    "\n",
    "Now that I've shown the scores,  I can add more images and retrain our classifier.  This is done much the same way the classifier is trainined to start with, and here I'm just using some more data and updating the classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_gr = PositiveClass(\"golden\", tempDir, [os.path.join(base_path_gr,x) for x in test_gr])\n",
    "\n",
    "update_gr_file = open(updated_gr.zipfile(),'rb')\n",
    "vr.update_classifier(classifier_id=my_classifier_id, golden_positive_examples=update_gr_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
